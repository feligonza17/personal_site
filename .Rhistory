max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
#Hacer asociasiones entre los términos
findFreqTerms(dtm, lowfreq = 4)
findAssocs(dtm, terms = "plata", corlimit = 0.2)
#Generar cloud
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
#Importarlo en la forma que se necesita para generar la nube de palabras
df_control <-
df %>%
filter(Grupo==2)
dfCorpus_control <- VCorpus(VectorSource(df_control$`Respuesta Cualitativa`))
df_control <-
df %>%
filter(Grupo==1)
dfCorpus_control <- VCorpus(VectorSource(df_control$`Respuesta Cualitativa`))
inspect(dfCorpus_control)
# Convert the text to lower case
dfCorpus_control <- tm_map(dfCorpus_control, content_transformer(tolower))
# Remove spanish common stopwords
dfCorpus_control <- tm_map(dfCorpus_control, removeWords, stopwords("spanish"))
# Remove your own stop word
# specify your stopwords as a character vector
dfCorpus_control <- tm_map(dfCorpus_control, removeWords, c("pues"))
# Remove punctuations
dfCorpus_control <- tm_map(dfCorpus_control, removePunctuation)
#Generar frecuencia de palabras
dtm <- TermDocumentMatrix(dfCorpus_control)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
df_control <-
df %>%
filter(Grupo==2)
dfCorpus_control <- VCorpus(VectorSource(df_control$`Respuesta Cualitativa`))
inspect(dfCorpus_control)
# Convert the text to lower case
dfCorpus_control <- tm_map(dfCorpus_control, content_transformer(tolower))
# Remove spanish common stopwords
dfCorpus_control <- tm_map(dfCorpus_control, removeWords, stopwords("spanish"))
# Remove your own stop word
# specify your stopwords as a character vector
dfCorpus_control <- tm_map(dfCorpus_control, removeWords, c("pues"))
# Remove punctuations
dfCorpus_control <- tm_map(dfCorpus_control, removePunctuation)
#Generar frecuencia de palabras
dtm <- TermDocumentMatrix(dfCorpus_control)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
#Generar cloud
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
findAssocs(dtm, terms = "plata", corlimit = 0.2)
findAssocs(dtm, terms = "plata", corlimit = 0.2)
#Hacer asociasiones entre los términos
findFreqTerms(dtm, lowfreq = 4)
findAssocs(dtm, terms = "plata", corlimit = 0.2)
findAssocs(dtm, terms = "padres", corlimit = 0.2)
findAssocs(dtm, terms = "plata", corlimit = 0.2)
df_control <-
df %>%
filter(Grupo==2)
dfCorpus_control <- VCorpus(VectorSource(df_control$`Respuesta Cualitativa`))
View(df_control)
# Convert the text to lower case
dfCorpus_control <- tm_map(dfCorpus_control, content_transformer(tolower))
# Remove spanish common stopwords
dfCorpus_control <- tm_map(dfCorpus_control, removeWords, stopwords("spanish"))
# Remove your own stop word
# specify your stopwords as a character vector
dfCorpus_control <- tm_map(dfCorpus_control, removeWords, c("pues"))
# Remove punctuations
dfCorpus_control <- tm_map(dfCorpus_control, removePunctuation)
#Generar frecuencia de palabras
dtm <- TermDocumentMatrix(dfCorpus_control)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
#Hacer asociasiones entre los términos
findFreqTerms(dtm, lowfreq = 4)
findAssocs(dtm, terms = "plata", corlimit = 0.2)
findAssocs(dtm, terms = "padres", corlimit = 0.2)
findAssocs(dtm, terms = "cuaderno", corlimit = 0.2)
devtools::install_github("kassambara/navdata")
if(!require(devtools)) install.packages("devtools")
?findAssocs
#Importarlo en la forma que se necesita para generar la nube de palabras
dfCorpus <- VCorpus(VectorSource(df$`Respuesta Cualitativa`))
inspect(dfCorpus)
# Convert the text to lower case
dfCorpus <- tm_map(dfCorpus, content_transformer(tolower))
# Remove spanish common stopwords
dfCorpus <- tm_map(dfCorpus, removeWords, stopwords("spanish"))
# Remove your own stop word
# specify your stopwords as a character vector
dfCorpus <- tm_map(dfCorpus, removeWords, c("pues"))
# Remove punctuations
dfCorpus <- tm_map(dfCorpus, removePunctuation)
#Generar frecuencia de palabras
dtm <- TermDocumentMatrix(dfCorpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
#Generar cloud
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
#Hacer asociasiones entre los términos
findFreqTerms(dtm, lowfreq = 4)
findAssocs(dtm, terms = c("plata","padres","amigo"), corlimit = 0.2)
#Importarlo en la forma que se necesita para generar la nube de palabras
df_alto <-
df %>%
filter(`Puntaje INSE`>39)
dfCorpus_alto <- VCorpus(VectorSource(df_alto$`Respuesta Cualitativa`))
inspect(dfCorpus_alto)
# Convert the text to lower case
dfCorpus_alto <- tm_map(dfCorpus_alto, content_transformer(tolower))
# Remove spanish common stopwords
dfCorpus_alto <- tm_map(dfCorpus_alto, removeWords, stopwords("spanish"))
# Remove your own stop word
# specify your stopwords as a character vector
dfCorpus_alto <- tm_map(dfCorpus_alto, removeWords, c("pues"))
# Remove punctuations
dfCorpus_alto <- tm_map(dfCorpus_alto, removePunctuation)
#Generar frecuencia de palabras
dtm <- TermDocumentMatrix(dfCorpus_alto)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
#Generar cloud
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
#Hacer asociasiones entre los términos
findFreqTerms(dtm, lowfreq = 4)
findAssocs(dtm, terms = c("plata","padres","amigo"), corlimit = 0.2)
#Importarlo en la forma que se necesita para generar la nube de palabras
df_bajo <-
df %>%
filter(`Puntaje INSE`<32)
dfCorpus_bajo <- VCorpus(VectorSource(df_bajo$`Respuesta Cualitativa`))
inspect(dfCorpus_bajo)
# Convert the text to lower case
dfCorpus_bajo <- tm_map(dfCorpus_bajo, content_transformer(tolower))
# Remove spanish common stopwords
dfCorpus_bajo <- tm_map(dfCorpus_bajo, removeWords, stopwords("spanish"))
# Remove your own stop word
# specify your stopwords as a character vector
dfCorpus_bajo <- tm_map(dfCorpus_bajo, removeWords, c("pues"))
# Remove punctuations
dfCorpus_bajo <- tm_map(dfCorpus_bajo, removePunctuation)
#Generar frecuencia de palabras
dtm <- TermDocumentMatrix(dfCorpus_bajo)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
#Generar cloud
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
#Hacer asociasiones entre los términos
findFreqTerms(dtm, lowfreq = 4)
findAssocs(dtm, terms = c("plata","padres","amigo"), corlimit = 0.2)
#Importarlo en la forma que se necesita para generar la nube de palabras
df_exp <-
df %>%
filter(Grupo==1)
dfCorpus_exp <- VCorpus(VectorSource(df_exp$`Respuesta Cualitativa`))
inspect(dfCorpus_exp)
# Convert the text to lower case
dfCorpus_exp <- tm_map(dfCorpus_exp, content_transformer(tolower))
# Remove spanish common stopwords
dfCorpus_exp <- tm_map(dfCorpus_exp, removeWords, stopwords("spanish"))
# Remove your own stop word
# specify your stopwords as a character vector
dfCorpus_exp <- tm_map(dfCorpus_exp, removeWords, c("pues"))
# Remove punctuations
dfCorpus_exp <- tm_map(dfCorpus_exp, removePunctuation)
#Generar frecuencia de palabras
dtm <- TermDocumentMatrix(dfCorpus_exp)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
#Hacer asociasiones entre los términos
findFreqTerms(dtm, lowfreq = 4)
findAssocs(dtm, terms = c("plata","padres","amigo"), corlimit = 0.2)
findAssocs(dtm, terms = c("plata","padres","amigo", "dinero"), corlimit = 0.2)
library(devtools)
install_github("cbail/textnets")
library(textnets)
View(df)
textnets::PrepText(df,grupovar=Grupo_Nombre,textvar = Respuesta Cualitativa, node_type="words") )
network_ancho <- textnets::PrepText(df,grupovar=Grupo_Nombre,textvar = Respuesta Cualitativa, node_type="words") )
network_ancho <- PrepText(df,grupovar=Grupo_Nombre,textvar = Respuesta Cualitativa, node_type="words") )
network_ancho <- PrepText(df,grupovar="Grupo_Nombre" ,textvar = "Respuesta Cualitativa", node_type="words") )
network_ancho <- PrepText(df,grupovar="Grupo_Nombre" ,textvar = "Respuesta Cualitativa", node_type="words")
network_ancho <- PrepText(df,grupvar="Grupo_Nombre" ,textvar = "Respuesta Cualitativa", node_type="words")
network_ancho <- PrepText(df, groupvar="Grupo_Nombre" ,textvar = "Respuesta Cualitativa", node_type="words")
network_ancho <- PrepText(df, groupvar="Grupo_Nombre" ,textvar = "Respuesta Cualitativa", node_type = "groups", tokenizer = "words", pos = "nouns", remove_stop_words = TRUE, compound_nouns = TRUE)
network_ancho <- PrepText(df, groupvar="Grupo_Nombre" ,textvar = df$`Respuesta Cualitativa`, node_type = "groups", tokenizer = "words", pos = "nouns", remove_stop_words = TRUE, compound_nouns = TRUE)
network_ancho <- PrepText(df, groupvar="Grupo_Nombre" ,textvar =`Respuesta Cualitativa`, node_type = "groups", tokenizer = "words", pos = "nouns", remove_stop_words = TRUE, compound_nouns = TRUE)
network_ancho <- PrepText(df, groupvar="Grupo_Nombre" ,textvar ="Respuesta Cualitativa, node_type = "groups", tokenizer = "words", pos = "nouns", remove_stop_words = TRUE, compound_nouns = TRUE)
network_ancho <- PrepText(df, groupvar="Grupo_Nombre" ,textvar ="Respuesta Cualitativa", node_type = "groups", tokenizer = "words", pos = "nouns", remove_stop_words = TRUE, compound_nouns = TRUE)
network_ancho <- PrepText(df, groupvar="Grupo_Nombre" ,textvar ="Respuesta cualitativa", node_type = "groups", tokenizer = "words", pos = "nouns", remove_stop_words = TRUE, compound_nouns = TRUE)
network_ancho <- PrepText(df, groupvar="Grupo_Nombre" ,textvar ="Respuesta Cualitativa", node_type = "groups", tokenizer = "words", pos = "nouns", remove_stop_words = TRUE, compound_nouns = TRUE)
colnames(df)["Respuesta Cualitativa"] <- "texto"
colnames(df)[13] <- "texto"
network_ancho <- PrepText(df, groupvar="Grupo_Nombre" ,textvar ="texto", node_type = "groups", tokenizer = "words", pos = "nouns", remove_stop_words = TRUE, compound_nouns = TRUE)
network_ancho <- PrepText(df, groupvar="Grupo_Nombre" ,textvar ="texto", node_type = "words", tokenizer = "words", pos = "nouns", remove_stop_words = TRUE, compound_nouns = TRUE)
segundo_paso <- CreateTextnet(network_ancho)
VisTextNet(segundo_paso, label_degree_cut = 0))
VisTextNet(segundo_paso, label_degree_cut = 0)
#Importarlo en la forma que se necesita para generar la nube de palabras
dfCorpus <- VCorpus(VectorSource(df$`Respuesta Cualitativa`))
#Importarlo en la forma que se necesita para generar la nube de palabras
df_bajo <-
df %>%
filter(`Puntaje INSE`<32)
network_ancho <- PrepText(df_bajo, groupvar="Grupo_Nombre" ,textvar ="texto", node_type = "words", tokenizer = "words", pos = "nouns", remove_stop_words = TRUE, compound_nouns = TRUE)
segundo_paso <- CreateTextnet(network_ancho)
VisTextNet(segundo_paso, label_degree_cut = 0)
network_ancho <- PrepText(df_exp, groupvar="Grupo_Nombre" ,textvar ="texto", node_type = "words", tokenizer = "words", pos = "nouns", remove_stop_words = TRUE, compound_nouns = TRUE)
network_ancho <- PrepText(df_exp, groupvar="Grupo_Nombre" ,textvar ="texto", node_type = "words", tokenizer = "words", pos = "nouns", remove_stop_words = TRUE, compound_nouns = TRUE)
#Importarlo en la forma que se necesita para generar la nube de palabras
df_exp <-
df %>%
filter(Grupo==1)
network_ancho <- PrepText(df_exp, groupvar="Grupo_Nombre" ,textvar ="texto", node_type = "words", tokenizer = "words", pos = "nouns", remove_stop_words = TRUE, compound_nouns = TRUE)
segundo_paso <- CreateTextnet(network_ancho)
VisTextNet(segundo_paso, label_degree_cut = 0)
segundo_paso <- CreateTextnet(network_ancho)
network_ancho <- PrepText(df_exp, groupvar="Colegio" ,textvar ="texto", node_type = "words", tokenizer = "words", pos = "nouns", remove_stop_words = TRUE, compound_nouns = TRUE)
segundo_paso <- CreateTextnet(network_ancho)
VisTextNet(segundo_paso, label_degree_cut = 0)
VisTextNet(segundo_paso, label_degree_cut = 3)
VisTextNet(segundo_paso, label_degree_cut = 10)
network_ancho <- PrepText(df, groupvar="Grupo_Nombre" ,textvar ="texto", node_type = "words", tokenizer = "words", pos = "nouns", remove_stop_words = TRUE, compound_nouns = TRUE)
segundo_paso <- CreateTextnet(network_ancho)
VisTextNet(segundo_paso, label_degree_cut = 6)
VisTextNet(segundo_paso, label_degree_cut = 15)
?PrepText
install.packages("htmlwidgets")
install.packages("htmlwidgets")
library(htmlwidgets)
vis <- VisTextNetD3(segundo_paso,
prune_cut=.50,
height=1000,
width=1400,
bound=FALSE,
zoom=TRUE,
charge=-30)
library(textnets)
install_github("cbail/textnets")
vis <- VisTextNetD3(segundo_paso,
prune_cut=.50,
height=1000,
width=1400,
bound=FALSE,
zoom=TRUE,
charge=-30)
vis <- VisTextNetD3(segundo_paso,
height=1000,
width=1400,
bound=FALSE,
zoom=TRUE,
charge=-30)
saveWidget(vis, "sotu_textnet.html")
network_ancho <- PrepText(df_bajo, groupvar="Grupo_Nombre" ,textvar ="texto", node_type = "words", tokenizer = "words", pos = "nouns", remove_stop_words = TRUE, compound_nouns = TRUE)
segundo_paso <- CreateTextnet(network_ancho)
VisTextNet(segundo_paso, label_degree_cut = 15)
vis <- VisTextNetD3(segundo_paso,
height=1000,
width=1400,
bound=FALSE,
zoom=TRUE,
charge=-30)
saveWidget(vis, "sotu_textnet.html")
df <- read_excel("Datos Análisis éste sí.xlsx")
# Load
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("tidyr")
library("dplyr")
library("readstata13", lib.loc="~/R/win-library/3.5")
library("readxl", lib.loc="~/R/win-library/3.5")
library("readxl")
df <- read_excel("Datos Análisis éste sí.xlsx")
#Importarlo en la forma que se necesita para generar la nube de palabras
dfCorpus <- VCorpus(VectorSource(df$`Respuesta Cualitativa`))
inspect(dfCorpus)
# Convert the text to lower case
dfCorpus <- tm_map(dfCorpus, content_transformer(tolower))
# Remove spanish common stopwords
dfCorpus <- tm_map(dfCorpus, removeWords, stopwords("spanish"))
# Remove your own stop word
# specify your stopwords as a character vector
dfCorpus <- tm_map(dfCorpus, removeWords, c("pues"))
# Remove punctuations
dfCorpus <- tm_map(dfCorpus, removePunctuation)
#Generar frecuencia de palabras
dtm <- TermDocumentMatrix(dfCorpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
#Hacer asociasiones entre los términos
findFreqTerms(dtm, lowfreq = 4)
findAssocs(dtm, terms = c("plata","padres","amigo"), corlimit = 0.2)
knitr::opts_chunk$set(echo = FALSE)
df <- read_excel("Datos Análisis éste sí.xlsx")
# Nube de palabras con TODOS los datos ------------------------------------
#Importarlo en la forma que se necesita para generar la nube de palabras
dfCorpus <- VCorpus(VectorSource(df$`Respuesta Cualitativa`))
# Convert the text to lower case
dfCorpus <- tm_map(dfCorpus, content_transformer(tolower))
# Remove spanish common stopwords
dfCorpus <- tm_map(dfCorpus, removeWords, stopwords("spanish"))
# Remove your own stop word
# specify your stopwords as a character vector
dfCorpus <- tm_map(dfCorpus, removeWords, c("pues"))
# Remove punctuations
dfCorpus <- tm_map(dfCorpus, removePunctuation)
#Generar frecuencia de palabras
dtm <- TermDocumentMatrix(dfCorpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
#Hacer asociasiones entre los términos
findAssocs(dtm, terms = c("plata","padres","amigo"), corlimit = 0.2)
plot(pressure)
knitr::opts_chunk$set(echo = FALSE)
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("tidyr")
library("dplyr")
library("readxl")
df <- read_excel("Datos Análisis éste sí.xlsx")
# Nube de palabras con TODOS los datos ------------------------------------
#Importarlo en la forma que se necesita para generar la nube de palabras
dfCorpus <- VCorpus(VectorSource(df$`Respuesta Cualitativa`))
# Convert the text to lower case
dfCorpus <- tm_map(dfCorpus, content_transformer(tolower))
# Remove spanish common stopwords
dfCorpus <- tm_map(dfCorpus, removeWords, stopwords("spanish"))
# Remove your own stop word
# specify your stopwords as a character vector
dfCorpus <- tm_map(dfCorpus, removeWords, c("pues"))
# Remove punctuations
dfCorpus <- tm_map(dfCorpus, removePunctuation)
#Generar frecuencia de palabras
dtm <- TermDocumentMatrix(dfCorpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
#Hacer asociasiones entre los términos
findAssocs(dtm, terms = c("plata","padres","amigo"), corlimit = 0.2)
# Nube de palabras con estrato alto ---------------------------------------
#Importarlo en la forma que se necesita para generar la nube de palabras
df_alto <-
df %>%
filter(`Puntaje INSE`>39)
dfCorpus_alto <- VCorpus(VectorSource(df_alto$`Respuesta Cualitativa`))
# Convert the text to lower case
dfCorpus_alto <- tm_map(dfCorpus_alto, content_transformer(tolower))
# Remove spanish common stopwords
dfCorpus_alto <- tm_map(dfCorpus_alto, removeWords, stopwords("spanish"))
# Remove your own stop word
# specify your stopwords as a character vector
dfCorpus_alto <- tm_map(dfCorpus_alto, removeWords, c("pues"))
# Remove punctuations
dfCorpus_alto <- tm_map(dfCorpus_alto, removePunctuation)
#Generar frecuencia de palabras
dtm <- TermDocumentMatrix(dfCorpus_alto)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
#Hacer asociasiones entre los términos
findAssocs(dtm, terms = c("plata","padres","amigo"), corlimit = 0.2)
# Asociaciones entre estrato bajo -----------------------------------------
#Importarlo en la forma que se necesita para generar la nube de palabras
df_bajo <-
df %>%
filter(`Puntaje INSE`<32)
dfCorpus_bajo <- VCorpus(VectorSource(df_bajo$`Respuesta Cualitativa`))
# Convert the text to lower case
dfCorpus_bajo <- tm_map(dfCorpus_bajo, content_transformer(tolower))
# Remove spanish common stopwords
dfCorpus_bajo <- tm_map(dfCorpus_bajo, removeWords, stopwords("spanish"))
# Remove your own stop word
# specify your stopwords as a character vector
dfCorpus_bajo <- tm_map(dfCorpus_bajo, removeWords, c("pues"))
# Remove punctuations
dfCorpus_bajo <- tm_map(dfCorpus_bajo, removePunctuation)
#Generar frecuencia de palabras
dtm <- TermDocumentMatrix(dfCorpus_bajo)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
#Hacer asociasiones entre los términos
findAssocs(dtm, terms = c("plata","padres","amigo"), corlimit = 0.2)
#Importarlo en la forma que se necesita para generar la nube de palabras
df_exp <-
df %>%
filter(Grupo==1)
dfCorpus_exp <- VCorpus(VectorSource(df_exp$`Respuesta Cualitativa`))
# Convert the text to lower case
dfCorpus_exp <- tm_map(dfCorpus_exp, content_transformer(tolower))
# Remove spanish common stopwords
dfCorpus_exp <- tm_map(dfCorpus_exp, removeWords, stopwords("spanish"))
# Remove your own stop word
# specify your stopwords as a character vector
dfCorpus_exp <- tm_map(dfCorpus_exp, removeWords, c("pues"))
# Remove punctuations
dfCorpus_exp <- tm_map(dfCorpus_exp, removePunctuation)
#Generar frecuencia de palabras
dtm <- TermDocumentMatrix(dfCorpus_exp)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
#Hacer asociasiones entre los términos
findAssocs(dtm, terms = c("plata","padres","amigo", "dinero"), corlimit = 0.2)
# Asociaciones entre grupo control (cuaderno)-----------------------------------------
#Importarlo en la forma que se necesita para generar la nube de palabras
df_control <-
df %>%
filter(Grupo==2)
dfCorpus_control <- VCorpus(VectorSource(df_control$`Respuesta Cualitativa`))
# Convert the text to lower case
dfCorpus_control <- tm_map(dfCorpus_control, content_transformer(tolower))
# Remove spanish common stopwords
dfCorpus_control <- tm_map(dfCorpus_control, removeWords, stopwords("spanish"))
# Remove your own stop word
# specify your stopwords as a character vector
dfCorpus_control <- tm_map(dfCorpus_control, removeWords, c("pues"))
# Remove punctuations
dfCorpus_control <- tm_map(dfCorpus_control, removePunctuation)
#Generar frecuencia de palabras
dtm <- TermDocumentMatrix(dfCorpus_control)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
#Hacer asociasiones entre los términos
findAssocs(dtm, terms = c("plata","padres","amigo", "dinero"), corlimit = 0.2)
plot(pressure)
install.packages("rmarkdown", type = "source")
install.packages("rmarkdown", type = "source")
touch _site.yml
touch "_site.yml"
_site.yml
setwd("C:/Users/felig/personal_site")
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("C:\Users\felig\personal_site")
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("\Users\felig\personal_site")
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("Users\felig\personal_site")
setwd("C:/Users/felig/personal_site")
#Set our working directory.
#This helps avoid confusion if our working directory is
#not our site because of other projects we were
#working on at the time.
setwd("C:/Users/felig/personal_site")
#render your sweet site.
rmarkdown::render_site()
#render your sweet site.
rmarkdown::render_site()
#render your sweet site.
rmarkdown::render_site()
#render your sweet site.
rmarkdown::render_site()
#render your sweet site.
rmarkdown::render_site()
#render your sweet site.
rmarkdown::render_site()
